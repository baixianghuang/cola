{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6cf5a0-7898-4c23-8d9d-7c5f81230b77",
   "metadata": {
    "tags": []
   },
   "source": [
    "# COLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853d84d-4fbe-4a33-b2d7-91514df36aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f314bad-2ab5-4d7e-a048-2a141a9e92f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_SIZE = 200\n",
    "random_seed = 7\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-05-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48293624-51cf-4000-a77a-cac9d724f80b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def language_check(df):\n",
    "    print('# en rows:', f\"{df[df.lang=='en'].shape[0]:,}\", ' % of English text:', f\"{df[df.lang=='en'].shape[0] / df.shape[0]}\")\n",
    "    df = df[df.lang=='en']\n",
    "    df.drop('lang', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def eval_fn(df):\n",
    "    print('Input shape:', df.shape)\n",
    "    df['same'] = df.author_id1 == df.author_id2\n",
    "    y_test, y_pred = df.same, df.binary\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)*100\n",
    "    precision = metrics.precision_score(y_test, y_pred)*100\n",
    "    recall = metrics.recall_score(y_test, y_pred)*100\n",
    "    f1 = metrics.f1_score(y_test, y_pred)*100\n",
    "    print('# same authors:', df['same'].sum())\n",
    "    print(\"Accuracy: %.2f\" % (acc), '%', end=' | ')   \n",
    "    print(\"Precision: %.2f\" % (precision), '%', end=' | ')\n",
    "    print(\"Recall: %.2f\" % (recall), '%', end=' | ')\n",
    "    print(\"F1: %.2f\" % (f1), '%\\n')\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31487113-93ce-4bf0-af9a-7c084d5c5ba3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chunked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84c96e-76bc-4607-9419-adcc5c747ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/data//dataset/'\n",
    "df = pd.read_csv(file_path+\"blog_test_en.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155a9bb-1419-46f0-a0b9-ae3e073b1f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462e5e9-1681-43f7-9601-00d2c0e4c83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'decoded_text': 'text'}, inplace=True)\n",
    "df.text = df.text.str.strip()\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a564e-6629-46d6-8169-3b24e6b4e433",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding and removing duplicate rows\n",
    "df[df[['text']].duplicated(keep=False)].sort_values('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd6039-72a6-494c-b169-18fe3ae302dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('# duplicates:', df.text.duplicated().sum(), 'sanity check:', df.shape[0] - len(set(df.text)))\n",
    "print('Before removing duplicates, df.shape:', df.shape)\n",
    "df = df.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
    "print('New df.shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5990d5d-a5c7-49a7-8ea3-b51cfd750348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import py3langid\n",
    "df['lang'] = df['text'].apply(lambda x: py3langid.classify(x)[0])\n",
    "print(f\"{df.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f09293-d9a1-4655-ab59-a2e09d8f93aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = language_check(df)\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d20c7-c515-44ee-a811-94a3f8efb658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = df.id.value_counts()\n",
    "df = df[df.id.isin(v[v > 10].index)]\n",
    "print('# unique authors', len(df.id.unique()))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd407d9-e510-41c2-a6c9-bb48097d7d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220848e-a66a-40af-b61f-4eac4dd47156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check # of tokens\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "for i in range(10):\n",
    "    text1, text2 = df.sample(2).text.values\n",
    "    print(num_tokens_from_string(text1 + text2, \"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab41b39-643e-4d8e-bed7-5691b61fb4f1",
   "metadata": {},
   "source": [
    "Consider if we use all different authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0e626-f04f-434a-8ac2-b5803095bc59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampler_fn(df, size=DATA_SIZE*5):\n",
    "    \"\"\"Sample a subset of df in a balanced way\"\"\"\n",
    "    dict_to_df = []\n",
    "    text_set = set()\n",
    "    author_set = set()\n",
    "\n",
    "    for i in range(size):\n",
    "        # print('Reptition No.', i)\n",
    "        if i % 2 == 0:  # sample documents from different authors\n",
    "            df_tmp = df.sample(2)  # should not use random_seed because we want different samples in each iteration\n",
    "            aut_id1, aut_id2 = df_tmp.id.tolist()\n",
    "            # while aut_id1 in author_set:\n",
    "            #     df_tmp = df.sample(2) \n",
    "            #     aut_id1, aut_id2 = df_tmp.id.tolist()\n",
    "            text1, text2 = df_tmp.text.tolist()\n",
    "            author_set.add(aut_id1)\n",
    "            author_set.add(aut_id2)\n",
    "        else:  # sample documents from same authors to make it balance\n",
    "            same_auth_id = df.sample(1).id.values[0]\n",
    "            while same_auth_id in author_set:\n",
    "                same_auth_id = df.sample(1).id.values[0]\n",
    "            aut_id1, aut_id2 = same_auth_id, same_auth_id\n",
    "            text1, text2 = df[df.id==same_auth_id].sample(2).text.tolist()\n",
    "            while text1 in text_set or text2 in text_set:\n",
    "                text1, text2 = df[df.id==same_auth_id].sample(2).text.tolist()\n",
    "        # print(text1, text2)\n",
    "        dict_row = {}\n",
    "        dict_row[\"text1\"], dict_row[\"text2\"] = text1, text2\n",
    "        dict_row[\"aut_id1\"], dict_row[\"aut_id2\"] = aut_id1, aut_id2\n",
    "        text_set.add(text1)\n",
    "        text_set.add(text2)\n",
    "        dict_to_df.append(dict_row)\n",
    "\n",
    "    df_sub = pd.DataFrame(dict_to_df)\n",
    "    df_sub['same'] = df_sub.aut_id1 == df_sub.aut_id2\n",
    "    print('# same authors:', df_sub['same'].sum(), '# different authors:', len(np.unique(df_sub.aut_id1)))\n",
    "    return df_sub\n",
    "    \n",
    "\n",
    "df_sub = sampler_fn(df)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855ce2e-45ca-45cf-b396-94825847d2bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub[df_sub[['text1']].duplicated(keep=False)].sort_values('text1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b3c63-b46b-41b1-9f9f-5c9a52ae955c",
   "metadata": {},
   "source": [
    "## Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1e3ce-36e3-4e52-b775-38fcee039f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt1 = r\"\"\"\n",
    "Your task is to verify if 2 input texts are written by a same author. Create a valid JSON object following this format: \\\n",
    "{\n",
    "    \"binary\": \"a boolean (True/False) indicating whether the input texts are written by a same author.\",\n",
    "    \"analysis\": \"an analysis to support the binary output above\",\n",
    "    \"confidence\": \"an integer score on a scale of 1-10 indicating how confident are you about the binary output.\",\n",
    "    \"similarity\": \"an integer score on a scale of 1-10 indicating the similarity between two texts.\",\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = r\"\"\"\n",
    "You are a linguist. Your task is to verify whether two input texts are written by the same author based on writing styles. Do not consider topic differences. \n",
    "\n",
    "Create a valid JSON object following this format: \\\n",
    "{\n",
    "    \"binary\": \"a boolean (True/False) indicating whether the input texts are written by a same author, make decision based on the linguistic analysis above.\",\n",
    "    \"analysis\": \"an analysis of linguistic similarities and differences indicating whether the input texts are written by a same author. Be specific about which linguistic features you analyze.\",\n",
    "    \"confidence\": \"an integer score on a scale of 1-10 indicating how confident are you about the binary output.\",\n",
    "    \"similarity\": \"an integer score on a scale of 1-10 indicating the stylistic similarity between the two texts; a high score suggests that the texts were written by the same author.\",\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "prompt3 = r\"\"\"\n",
    "You are a linguist. Your task is to verify whether two input texts are written by the same author based on writing styles. Do not consider topic differences.\n",
    "\n",
    "Create a valid JSON object for authorship verification following this format:\n",
    "{\n",
    "    \"binary\": \" a boolean (True/False) indicating whether the input texts are written by the same author. Decisions should be made based on analysis above and not on content differences.\",\n",
    "    \"analysis\": \"an analysis of the grammer style of the input texts that support the binary output. Be specific and in detail.\",\n",
    "    \"confidence\": \"an integer score on a scale of 1-10, representing how confident you are about the binary output.\",\n",
    "    \"similarity\": \"an integer score on a scale of 1-10 indicating the style similarity between two texts; a high score suggests that the texts were written by the same author.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db50b46-d870-476d-9740-e6b2a0367942",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_verfication(df_sub, model_name, prompt_prefix, data_size, prompt_postfix=\"\", deployment_id=\"test\", temperature=0):\n",
    "    ls = []\n",
    "    err_ls = []\n",
    "    if model_name == 'gpt-4':\n",
    "        openai.api_key = \"\" \n",
    "        openai.api_base = \"\"\n",
    "    elif model_name == 'gpt-35-turbo-16k':\n",
    "        openai.api_key = \"\"  # \n",
    "        openai.api_base = \"\"\n",
    "\n",
    "    for i0, i in enumerate(df_sub.index[:data_size]):\n",
    "        print('zero-indexed:', i0, ' index:', i)\n",
    "        aut_id1, aut_id2 = df_sub.loc[i, 'aut_id1'], df_sub.loc[i, 'aut_id2']\n",
    "        text1, text2 = df_sub.loc[i, 'text1'], df_sub.loc[i, 'text2']\n",
    "        # print(text1, text2)\n",
    "\n",
    "        prompt = prompt_prefix + f\"\"\"The input texts (text1 and text2) are delimited with triple backticks. Input texts: ```text1: {text1}, text2: {text2}```\\\n",
    "        Do not generate other word\"\"\" + prompt_postfix\n",
    "        raw_response = openai.ChatCompletion.create(deployment_id=deployment_id, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=temperature)\n",
    "        response_str = raw_response.choices[0].message[\"content\"]\n",
    "        print('Raw response content:', response_str, '\\n')\n",
    "        \n",
    "        # To avoid JSONDecodeError: Expecting ',' delimiter\n",
    "        sub_str1, sub_str2 = '\"analysis\": \"', '\",\\n    \"confidence\"'\n",
    "        if sub_str2 not in response_str:\n",
    "            sub_str2 = '\", \"confidence\"'\n",
    "        try:\n",
    "            idx1, idx2 = response_str.index(sub_str1), response_str.index(sub_str2)\n",
    "            # print(response_str[idx1+len(sub_str1):idx2])\n",
    "            evidence_mod = response_str[idx1+len(sub_str1):idx2].replace('\"', '\\\\\"')\n",
    "            str_mod = response_str[:idx1+len(sub_str1)] + evidence_mod + response_str[idx2:]\n",
    "            response = json.loads(str_mod, strict=False)  \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"===== JSONDecodeError =====\\n\")\n",
    "            err_ls.append(i)\n",
    "            continue\n",
    "        except ValueError:\n",
    "            print(f\"ValueError.\\n\")\n",
    "            err_ls.append(i)\n",
    "            continue\n",
    "        # print(str_mod, '\\n')\n",
    "        # response = json.loads(str_mod)\n",
    "        # print(response, '\\n')\n",
    "        \n",
    "        response[\"text1\"], response[\"text2\"] = text1, text2\n",
    "        response[\"author_id1\"], response[\"author_id2\"] = aut_id1, aut_id2\n",
    "        response[\"model\"] = raw_response[\"model\"]\n",
    "        response[\"tokens\"] = raw_response[\"usage\"][\"total_tokens\"]\n",
    "        ls.append(response)\n",
    "        response = None\n",
    "    print('# errors', len(err_ls), '\\n')\n",
    "    df_res = pd.DataFrame(ls)\n",
    "    eval_fn(df_res)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2306a6-989e-40ea-8773-9e9fa2664dbc",
   "metadata": {},
   "source": [
    "### No guidance at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c52a3-63c1-4746-a37c-ef71e268c5e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df1_gpt35 = run_verfication(df_sub, model_name='gpt-35-turbo-16k', prompt_prefix=prompt1, data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ca2a1-b997-48d6-94d9-20b6850de8d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df1_gpt4 = run_verfication(df_sub, model_name='gpt-4', prompt_prefix=prompt1, data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d2665-35eb-43dd-8d84-edeb85202d7c",
   "metadata": {},
   "source": [
    "### No guidance + CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23b55d-895c-4e3d-bcbc-ed9219a4991f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df1_gpt35_cot = run_verfication(df_sub[:], model_name='gpt-35-turbo-16k', prompt_prefix=prompt1, data_size=DATA_SIZE, prompt_postfix=\" Let's think step by step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd23706-03bb-4cbc-9365-f5d4b690e2cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df1_gpt4_cot = run_verfication(df_sub, model_name='gpt-4', prompt_prefix=prompt1, data_size=DATA_SIZE, prompt_postfix=\" Let's think step by step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a26e4-a53b-4df3-aa90-c3cf9142b0f5",
   "metadata": {},
   "source": [
    "### V2: some high-level guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72228043-0107-44d5-9266-b052884fc832",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df2_gpt35 = run_verfication(df_sub, model_name='gpt-35-turbo-16k', prompt_prefix=prompt2, data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d5b17-c30c-4c6f-a1ec-a9a7178a73db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df2_gpt4 = run_verfication(df_sub, model_name='gpt-4', prompt_prefix=prompt2, data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cf6c7-332d-4323-8c0d-b842e1409eb0",
   "metadata": {},
   "source": [
    "### Prompt V3: grammar as guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e1375-85bd-4ca4-a2d5-039b4a6907a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df3_gpt35 = run_verfication(df_sub, model_name='gpt-35-turbo-16k', prompt_prefix=prompt3, data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0375ea5-bba7-4a54-bb7c-794202fa111b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df3_gpt4 = run_verfication(df_sub, model_name='gpt-4', prompt_prefix=prompt3, data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa48fc-03af-4b5b-a626-04434f12d3e0",
   "metadata": {},
   "source": [
    "### COLA Prompt V4: with more explicit linguistic guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170ee68-d928-4d17-92c1-834c847bd55e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df4_gpt35 = run_verfication(df_sub[:], model_name='gpt-35-turbo-16k', prompt_prefix=prompt4, data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d90e18-5378-49d4-aa07-c99a4d761b36",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df4_gpt4 = run_verfication(df_sub, model_name='gpt-4', prompt_prefix=prompt4, data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903d6ab-ba75-4f8d-a99b-8520f274650b",
   "metadata": {},
   "source": [
    "### Prompt V5: linguistic feature list + CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f70aa9-db16-4423-acb9-52b6bede21f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df4_gpt35_cot = run_verfication(df_sub, model_name='gpt-35-turbo-16k', prompt_prefix=prompt4, data_size=DATA_SIZE, prompt_postfix=\" Let's think step by step. Only output the JSON string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f52b6-924a-44b0-8add-f39ebe6f0070",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df4_gpt4_cot = run_verfication(df_sub, model_name='gpt-4', prompt_prefix=prompt4, data_size=DATA_SIZE, prompt_postfix=\" Let's think step by step. Only output the JSON string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31048882-127d-4d73-8432-74a4c910c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4 = r\"\"\"\n",
    "You are a linguist and an authorship attribution expert. Your task is to verify whether two input texts are written by the same author based on their writing styles.\n",
    "Analyze the writing style based on the following linguistic feature list: Phrasal verbs (e.g., blow up), Modal verbs (e.g., might, may), Punctuation, Rare words, Affixes \\\n",
    "(e.g., -ation, -ification), Quantities (e.g., a lot, many), Humor, Sarcasm, Typographical errors, and Misspellings. Decisions should be based on these linguistic \\\n",
    "features and not on content or discourse differences, as the same author can write about different topics and employ different discourse types.\n",
    "Create a valid JSON object for authorship verification using this format:\n",
    "\n",
    "{\n",
    "    \"analysis\": \"An analysis of the features from the linguistic feature list to support your claim. Be specific about which part of the text indicates which linguistic feature, suggesting different authorship. Consider each linguistic feature individually.\",\n",
    "    \"binary\": \"A boolean (True/False) indicating whether the input texts are written by the same author. Decisions should be made based on the analysis above and not on content or discourse differences.\",\n",
    "    \"confidence\": \"An integer score on a scale of 1-10, representing how confident you are about the binary output.\",\n",
    "    \"similarity\": \"An integer score on a scale of 1-10 indicating the stylistic similarity between the two texts; a high score suggests that the texts were written by the same author.\",\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda1aa0-5b67-484e-ab30-0d577abbf786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_few_shot(df_sub, model_name, prompt_prefix, data_size, k, deployment_id=\"test\", temperature=0):\n",
    "    \"\"\"k: number of extra example from the same author for k-shot prompting\"\"\"\n",
    "    ls = []\n",
    "    err_ls = []\n",
    "    if model_name == 'gpt-4':\n",
    "        openai.api_key = \"d3c67b9fdf1e4f438d2ac07193c88708\" \n",
    "        openai.api_base = \"https://test-gpt-4-ks.openai.azure.com\"\n",
    "    elif model_name == 'gpt-35-turbo-16k':\n",
    "        openai.api_key = \"08e99b6c65e84ead8676c505ee4d6f1e\"  # a2e5095fc4a6420e873cdfddcaf46915\n",
    "        openai.api_base = \"https://iarpa.openai.azure.com\"\n",
    "\n",
    "    for i0, i in enumerate(df_sub.index[:data_size]):\n",
    "        print('zero-indexed:', i0, ' index:', i)\n",
    "        aut_id1, aut_id2 = df_sub.loc[i, 'aut_id1'], df_sub.loc[i, 'aut_id2']\n",
    "        text1, text2 = df_sub.loc[i, 'text1'], df_sub.loc[i, 'text2']\n",
    "        # print(text1, text2)\n",
    "        k_shot = df[df.id == aut_id1][:k]\n",
    "        k_texts = k_shot.text.tolist()\n",
    "\n",
    "        prompt = prompt_prefix + f\"\"\"The input texts are delimited with triple backticks: ```Example text: {text1+\". Example text from same author \".join(k_texts)}, query text: {text2}```\\\n",
    "        Do not generate other word\"\"\"\n",
    "        # prompt = prompt_prefix + f\"\"\"The input texts are delimited with triple backticks: ```{k+1} example texts written by the same author: {text1+k_texts}, query text: {text2}```\\\n",
    "        # Do not generate other word\"\"\"\n",
    "        raw_response = openai.ChatCompletion.create(deployment_id=deployment_id, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=temperature)\n",
    "        response_str = raw_response.choices[0].message[\"content\"]\n",
    "        print(response_str, '\\n')\n",
    "\n",
    "        # To avoid JSONDecodeError: Expecting ',' delimiter\n",
    "        sub_str1, sub_str2 = '\"analysis\": \"', '\", \"binary\"'\n",
    "        if sub_str2 not in response_str:\n",
    "            sub_str2 = '\",\\n  \"binary\"'\n",
    "        if sub_str2 not in response_str:\n",
    "            sub_str2 = '\",\\n\"binary\"'\n",
    "        if sub_str2 not in response_str:\n",
    "            sub_str2 = '\",\\n    \"binary\"' \n",
    "\n",
    "        try:\n",
    "            idx1, idx2 = response_str.index(sub_str1), response_str.index(sub_str2)\n",
    "            # print(response_str[idx1+len(sub_str1):idx2])\n",
    "            evidence_mod = response_str[idx1+len(sub_str1):idx2].replace('\"', '\\\\\"')\n",
    "            str_mod = response_str[:idx1+len(sub_str1)] + evidence_mod + response_str[idx2:]\n",
    "        \n",
    "            response = json.loads(str_mod)  \n",
    "        # except json.JSONDecodeError:\n",
    "        #     print(f\"JSONDecodeError.\\n\")\n",
    "        #     err_ls.append(i)\n",
    "        #     continue\n",
    "        except ValueError:\n",
    "            print(f\"ValueError.\\n\")\n",
    "            err_ls.append(i)\n",
    "            continue\n",
    "        # print(response, '\\n')\n",
    "        response[\"text1\"], response[\"text2\"] = text1, text2\n",
    "        response[\"author_id_text1\"], response[\"author_id_text2\"] = aut_id1, aut_id2\n",
    "        response[\"model\"] = raw_response[\"model\"]\n",
    "        response[\"tokens\"] = raw_response[\"usage\"][\"total_tokens\"]\n",
    "        ls.append(response)\n",
    "        response = None\n",
    "    return pd.DataFrame(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53986b0e-a3ad-4da3-be22-559b8dd1d46f",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "https://github.com/pan-webis-de/pan-code/blob/master/clef23/authorship-verification/pan23-verif-baseline-compressor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5b8a2-35d8-42f7-9312-c6ca470164c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_to_df = []\n",
    "\n",
    "for i in range(100):\n",
    "    df_tmp = df.sample(2)  # should not use random_seed because we want different samples in each iteration\n",
    "    aut_id1, aut_id2 = df_tmp.id.tolist()\n",
    "    text1, text2 = df_tmp.text.tolist()\n",
    "    dict_row = {}\n",
    "    dict_row[\"text1\"], dict_row[\"text2\"] = text1, text2\n",
    "    dict_row[\"aut_id1\"], dict_row[\"aut_id2\"] = aut_id1, aut_id2\n",
    "    dict_to_df.append(dict_row)\n",
    "\n",
    "df_imbalanced = pd.DataFrame(dict_to_df)\n",
    "df_imbalanced['same'] = df_imbalanced.aut_id1 == df_imbalanced.aut_id2\n",
    "print('# same authors:', df_imbalanced['same'].sum(), '# different authors:', len(np.unique(df_imbalanced.aut_id1)))\n",
    "print('Null Accuracy:', 1 - df_imbalanced['same'].sum() / df_imbalanced.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8c6f1-db83-4947-b916-05b44bf81524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.same.sum() / df_sub.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533254b-df69-413a-9b60-ace8f9741eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "from pan_verify_evaluator import evaluate_all\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c2e58-96cc-48aa-9d54-d1c6edcc3d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c2cd9-5304-41af-8329-21dd925bbeff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binarize(y, threshold=0.5, triple_valued=False):\n",
    "    y = np.array(y)\n",
    "    y = np.ma.fix_invalid(y, fill_value=threshold)\n",
    "    if triple_valued:\n",
    "        y[y > threshold] = 1\n",
    "    else:\n",
    "        y[y >= threshold] = 1\n",
    "    y[y < threshold] = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda06c7-99b9-4640-a1af-f335026c68b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def rescale(value, orig_min, orig_max, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Rescales a `value` in the old range defined by\n",
    "    `orig_min` and `orig_max`, to the new range\n",
    "    `new_min` and `new_max`. Assumes that\n",
    "    `orig_min` <= value <= `orig_max`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    value: float, default=None\n",
    "        The value to be rescaled.\n",
    "    orig_min: float, default=None\n",
    "        The minimum of the original range.\n",
    "    orig_max: float, default=None\n",
    "        The minimum of the original range.\n",
    "    new_min: float, default=None\n",
    "        The minimum of the new range.\n",
    "    new_max: float, default=None\n",
    "        The minimum of the new range.\n",
    "    Returns\n",
    "    ----------\n",
    "    new_value: float\n",
    "        The rescaled value.\n",
    "    \"\"\"\n",
    "\n",
    "    orig_span = orig_max - orig_min\n",
    "    new_span = new_max - new_min\n",
    "\n",
    "    try:\n",
    "        scaled_value = float(value - orig_min) / float(orig_span)\n",
    "    except ZeroDivisionError:\n",
    "        orig_span += 1e-6\n",
    "        scaled_value = float(value - orig_min) / float(orig_span)\n",
    "\n",
    "    return new_min + (scaled_value * new_span)\n",
    "\n",
    "\n",
    "def correct_scores(scores, p1, p2):\n",
    "    for sc in scores:\n",
    "        if sc <= p1:\n",
    "            yield rescale(sc, 0, p1, 0, 0.49)\n",
    "        elif p1 < sc < p2:\n",
    "            yield 0.5\n",
    "        else:\n",
    "            yield rescale(sc, p2, 1, 0.51, 1)  # np.array(list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08351bd5-41ee-479c-8245-385eaa0f9a82",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239842a-dfc7-4bbe-b583-2a3d0be2d288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_baseline(df, rescale_flag=True, vocab_size=3000, ngram_size=4, threshold_mv=True):\n",
    "    \"\"\"\" \n",
    "    the cosine similarity of two documents will range from 0 to 1, since the term frequencies\n",
    "    (using tf–idf weights) cannot be negative. \n",
    "    \"\"\"\n",
    "    print('-> load the model')\n",
    "    vectorizer = TfidfVectorizer(max_features=vocab_size, analyzer='char', ngram_range=(ngram_size, ngram_size))\n",
    "    print('-> calculating pairwise similarities')\n",
    "    similarities, labels = [], []\n",
    "    \n",
    "    for i in df.index:\n",
    "        x1, x2 = vectorizer.fit_transform(df.loc[i, ['text1', 'text2']]).toarray()\n",
    "        similarities.append(cosine_sim(x1, x2))\n",
    "        labels.append(int(df.loc[i, 'same']))\n",
    "    \n",
    "    similarities = np.array(similarities, dtype=np.float64)\n",
    "    labels = np.array(labels, dtype=np.float64)\n",
    "    print('cos similarity min and max:', np.min(similarities), np.max(similarities))\n",
    "    if rescale_flag:\n",
    "        # similarities = np.vectorize(rescale)(similarities, -1, 1, 0, 1)\n",
    "        rescale_similarities = (similarities + 1) / 2\n",
    "        print('rescaled cos similarity min and max:', np.min(rescale_similarities), np.max(rescale_similarities))\n",
    "    print('score:', evaluate_all(pred_y=similarities, true_y=labels))\n",
    "    adjusted = (similarities >= 0.5) * 1\n",
    "    print('sklearn F1:', metrics.f1_score(labels, adjusted))\n",
    "    print('\\nscore after resclae:', evaluate_all(pred_y=rescale_similarities, true_y=labels))\n",
    "    \n",
    "    if threshold_mv:\n",
    "        print('\\n-> determining optimal threshold')\n",
    "        scores = []\n",
    "        for th in np.linspace(0.05, 0.95, 100):  # \n",
    "            adjusted = (similarities >= th)\n",
    "            scores.append((th, metrics.f1_score(labels, adjusted),\n",
    "                           metrics.precision_score(labels, adjusted),\n",
    "                           metrics.recall_score(labels, adjusted)))\n",
    "        thresholds, f1s, precisions, recalls = zip(*scores)\n",
    "\n",
    "        max_idx = np.array(f1s).argmax()\n",
    "        max_f1 = f1s[max_idx]\n",
    "        max_th = thresholds[max_idx]\n",
    "        print(f'Dev results -> F1={max_f1} at th={max_th}')\n",
    "        \n",
    "        adjusted_new = (similarities >= max_th)\n",
    "    else:\n",
    "        adjusted_new = (similarities >= 0.5)\n",
    "    y_test, y_pred = labels, adjusted_new\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)*100\n",
    "    precision = metrics.precision_score(y_test, y_pred)*100\n",
    "    recall = metrics.recall_score(y_test, y_pred)*100\n",
    "    f1 = metrics.f1_score(y_test, y_pred)*100\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475eb65d-579d-4997-a98c-7627c13b2be4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "eval_baseline(df_sub[:DATA_SIZE], rescale_flag=True, threshold_mv=False)\n",
    "eval_baseline(df_sub[:DATA_SIZE], rescale_flag=True, threshold_mv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f139475-bfb7-4cd2-983f-5e09799e2161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8e1d3-bbd8-4422-9686-2ad51215649d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
